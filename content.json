{"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"Structral Deep Clustering Network","text":"Structral Deep Clustering NetworkWWW2020本文为尝试将结构化信息建模在深度聚类任务中，并提出了全新的结构化深度聚类模型，其中包括一个DNN模块、GCN模块和双重自监督模块，并通过一个delivery operator成功将autoencoder和GCN所学到的表示结合在一起，并从理论上证明了其有效性。Paper ABSTRACT深度聚类算法目前取得了不错的效果，但是大多数依赖于深度模型的表征能力，比如自编码器（autoencoder），这类深度聚类方法的能力在于提取数据本身的表示，而缺少对数据的结构的建模，本文基于GCN提出Structral Deep Clustering Network(SDCN)，利用delivery operator将自编码器中学到的表示传入对应GCN层中，然后通过一个双自监督机制将两个模型进行整合并一起更新，从而将结构信息融合到自编码器所生成的表示中。本文还对所提出的delivery operator进行理论分析，在这个算子下，GCN作为自编码器的高阶图正则化约束（high-order graph regularization constraint），自编码器可以缓解GCN过平滑（over-smoothing）的问题。在多个任务上取得了SOTA的效果。 INTRODUCTION聚类任务实质自安于将相似样本归于同一类，深度聚类的基本思想在于将聚类任务和深度学习的表征能力结合，通过学习更有效的数据表示来提升聚类效果，类似的方法只考虑数据本身的特征却忽略了数据的结构信息。文中提出在深度聚类中结合结构化信息有以下两个难点： 哪些结构化信息需要被考虑？低阶与高阶邻居 结构化信息和深度聚类之间的关系？DNN每层与多种类型结构信息之间的联系为了捕获数据的结构化信息，我们首先用KNN构图，用GCN去捕获KNN图中的低阶和高阶的结构信息，从而获得GCN-specific表示。为了在深度聚类中引入结构化信息，我们利用autoencoder模块捕捉数据的表示，并提出delivery operator将从GCN中或得到的表示与autoencoder获得的表示进行整合，同时我们证明在delivery operator的帮助下，GCN可以为autoencoder提供二阶图正则化，autoencoder则减轻了GCN过平滑的问题。由于两个模块均能输出数据表示，我们提出双自监督模块对GCN和autoencoder模块进行统一训练，实验表明提出的SDCN在六个数据集上取得了SOTA效果。 THE PROPOSED MODEL KNN Graph这一部分就是利用KNN对每个样本进行计算相似度，对每个样本取top-K的邻居生成相应的边。我们用了两种方法计算样本之间的相似度： 1) Heat Kernel:$$ S_{ij}=e^{- \\frac{||x_{i}-x_{j}||^{2}}{t}}$$2) Dot-product:$$ S_{ij}=x_{j}^T * x_{i} $$ DNN Module为了通用性，在本文中我们仅使用basic autoencoder从raw data中获取表示以适应不同的数据特征。最后将decoder的输出与原始数据的输入进行构造重构误差 GCN ModuleL层autoencoder获得到多种数据表示$\\mathbf{H}^1, \\mathbf{H}^2, \\cdots, \\mathbf{H}^L$，但是忽略了样本之间的联系，本模块用GCN来捕捉结构信息，我们认为每一层autoencoder的输出都包含了不同的信息且能够对原始数据进行重构，于是使用一个超参$\\epsilon=0.5$对两个模型所获取到的表示进行融合将最后一层的输出投射到相应的label维度，然后利用softmax进行归一化。 Dual Self-Supervised Module我们虽然已经将autoencoder和GCN整合在一起，但是还未针对深度聚类任务进行构造，通常autoencoder适用于无监督任务，而GCN则应用于半监督任务，因此不能直接应用到聚类问题上。我们提出一个双自监督模块，对两者进行整合，并进行端到端训练用于聚类任务。对于$i-th$样本和$j-th$集簇，使用学生t-分布作为相似度计算核函数，从而计算节点与簇中央节点（预训练autoencoder的表示经过K-Means获得）的相似度。（注意$\\mathbf{H}$是从autoencoder中学习到的表示，而$\\mathbf{Z}$是GCN模块输出的表示）$q_{ij}$可以视为样本$i$分到$j$集簇的概率，我们将$Q=[q_{ij}]$作为所有样本指派的分布，然后我们将利用KL Divergence对表示进行更新使$Q$分布趋近于$P$分布（自监督，因为P分布是从Q分布中获得，又监督Q分布的更新过程）$P$分布的目的让节点表示趋近于其集簇中央，是集簇更紧密，$f_{j}=\\sum_{i}{q_{ij}}$对于训练GCN模块，如果将上面集簇的指派作为真实标签，容易带来噪音和Trivial Solution，使整个模型训练崩塌，因此我们可以让GCN最后一层输出（分类层）的$Z$分布同时也趋近与$P$分布，同样也利用KL Divergence。这里使用KL散度作为目标函数有两个好处： 传统的多分类损失函数相比，KL散度可以更缓和的更新整个模型，避免嵌入有过大的波动 GCN和DNN整合使用同一个优化目标，让训练过程更加一致 模型最终的损失函数如下所示：最后将模型GCN模块的最终输出Z作为最终的聚类结果，因为GCN中包含了结构信息和数据自身特征 Theory Analysis暂略 Experiments我们对6个数据集进行了实验，包括USPS、HHAR、Reuters、ACM、DBLP、Citeseer，对比了6个baseline，并在四个指标上进行了实验，实验结果如下所示 Analysis https://dannywu1996.github.io/2020/02/27/structral-deep-clustering-network/#moreof Clustering Results 对于ACM、DBLP、Citeseer直接用原始图作为输入，可以从表格中看到提出的SDCN在4个指标上都要超越其他baseline， SDCN普遍高于$SDCN_Q$，表明结构化信息的作用，但同时也依赖于一个噪音少的初始网络 对于已经有网络结构的，GCN类的方法要优于基于自编码器的方法，否则相反。构造好的网络很重要！！ Analysis of Different Propagation Layers通过固定住autoencoder的层数4层去观察，GCN的层数对模型性能的影响： 增加GCN层数提升了聚类的性能，并且4层GCN效果最好，因为自编码器每一层所捕获的信息不同，4层GCN也能保存对应每一层自编码器所学到的表示 3层效果普遍比如2层，因为3层GCN用使用到第二层自编码器的结果，是一个转移层，信息有所丢失，且2层GCN的过平滑效果不严重K-sensitivity Analysis不同的$K$值效果有差异，但是普遍在$k=3$或$k=5$时取得最好的效果，$k=1$时缺少结构信息，$k=10$时社区之间重叠程度过高。","link":"/structral-deep-clustering-network.html"},{"title":"机器学习相关数学笔记","text":"机器学习中所涉及到数学知识接下来持续更新会对看论文过程中所遇到的一些不理解的数学知识进行汇总，方便以后进行查阅 Contraction Mapping Mutual Information Contraction Mapping一个收缩映射（contraction mapping）可以看做在度量空间（metric space） $(M,d)$中从$M$映射到自己本身的函数$f$，其满足对于任意$x \\in M, y \\in M$,$$d(f(x), f(y)) \\le kd(x,y), \\tag{1}$$其中$0 \\le k \\le 1$，$k$最小的那个值被叫做该映射$f$的Lipschitz constant。同样这个概念可以推广到任意两个度量空间$(M, d), (N, d’)$上，对于$f\\colon M\\rightarrow N$，$f$为收缩映射时满足存在一个常量$k&lt;1$，使所有$x,y \\in M$满足$$d’(f(x), f(y)) \\le kd(x,y) \\tag{2}$$ Mutual Information Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable. 总的来说互信息就是为了衡量两个随机变量之间的依赖性，对于随机变量$X$和$Y$，他们之间的互信息$I(X;Y)$可以用以下公式来计算：$$I(X;Y)=H(X)-H(X|Y) \\tag{3}$$其中$H(X)$为随机变量$X$的熵(entropy)，$H(X|Y)$为给定随机变量$Y$后$X$的条件熵，显然如果两个随机变量互相独立，则其互信息为$0$，给定$Y$的信息不会对$X$减少任何不确定性，互信息越大，意味着两个随机变量之间的依赖程度越高。","link":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%95%B0%E5%AD%A6.html"},{"title":"hexo_profile","text":"第一次创建自己的博客记录一下最终还是决定把自己的博客给建立起来，也希望是对自己的一个督促吧，也开始学习一下markdown的语法，加油","link":"/hexo-profile.html"},{"title":"聚类外部指标总结","text":"4种外部指标总结项目中要用到NMI和ARI两个指标，但是一直不太理解这两个指标究竟侧重点是什么，正好做一下记录，顺便对另外两种外部指标也进行一个调研(Purity and F-Measures） Purity的定义Purity的纯净度其实就是对划定好的聚类结果，对每个cluster中数目最多的一类作为该集簇的类别，从而计算样本正确分配到相应集簇的数目 但如果集簇的数目很多时，很容易取得较高purity，比如如果每一个样本自为一类则纯净度为1，因此我们不能用纯净度作为集簇的质量和数目的tradeoff。 NMI的定义nmi主要是用来通过互信息和熵值得比值，对于$\\Omega$为划分的集合，$\\mathbb{C}$为总共的类别， ARI的定义 rule of thumb is : Use ARI when the ground truth clustering has large equal sized clusters 介绍ARI之前先介绍一个Rand Index(RI), 假设一个集合$S={o_1, \\cdots, o_n}$和两个划分$X={X_1, \\cdots, X_r}$与$Y={Y_1, \\cdots, Y_s}$，可以获得以下四个统计量: $a$ 用来表示集合$S$中节点对若在$X$中属于同一划分同时在$Y$中也属于同一划分的数目 $b$ 用来表示集合$S$中节点对若在$X$中属于不同划分同时在$Y$中也属于不同划分的数目 $c$ 用来表示集合$S$中节点对若在$X$中属于同一个划分同时在$Y$中属于不同划分的数目 $d$ 用来表示集合$S$中节点对若在$X$中属于不同划分同时在$Y$中也属于同一划分的数目 则RI指标为:$$RI=\\frac{a+b}{a+b+c+d}=\\frac{a+b}{C_{n}^{2}}$$对于ARI来说其公式首先涉及到一个contigency table表格中的$n_{ij}$代表公共元素个数对于子集$X_i$与$Y_j$: $|X_{i} \\cup Y_{j}|$, 随后ARI的计算公式为","link":"/difference-between-nmi-and-ari.html"},{"title":"random_walks_and_markov_chains","text":"Random Walks and Markov ChainsRelated Concept strongly connected: 图中每个节点均能被其他节点可达（reachabel)","link":"/random-walks-and-markov-chains.html"},{"title":"Heterogeneous Graph Transformer论文分享","text":"Heterogeneous Graph Transformer本文提出了异质图上transformer结构HGT，保留节点特征分布异质性，提出元关系，并使用基于元关系计算相似度替代点乘，模型中利用相对时间差对动态图进行建模并提出HGSampling方法以解决规模大的图结构，在多个节点分类与链接预测任务上相较其他HGNN取得SOTA效果。paper ABSTRACT近年来提出的GNN多应用于同质图，考虑节点和边都属于同一种类型，而我们提出Heterogeneous Graph Transformer (HGT)模型可以解决Web-scale的异质图，为了捕获其异质性，我们设计了节点类型与边类型依赖的参数已使用异质图注意力机制，使模型维护了不同类型节点和边的专用表示。为了应对动态异质图，我们提出了相对时间编码机制来捕捉任意时刻的动态结构依赖性。同时我们设计了异质图mini-batch采样算法HGSampling使模型更高效。多种下游任务表明我们模型超越多个SOTA GNN baseline。 1. Introduction传统的异质图方法，如PathSim，metapath2vec等都需要通过预先定义meta-path元路径来建模异质结构，而近来也有将GNN应用到异质图中的工作，但他们也有以下问题： 大多数GNN也依赖于预先定于元路径，需要专家经验 通常这些GNN模型假设所有节点和边特征均属于同一个特征空间，或者为每一种类型设定non-sharing的投射矩阵，导致这些模型无法充分捕捉图上的异质性 通常忽略了异质图的动态性 没法应用于大规模的异质图中 比如OAG(Open Acadamic Graph)中每个节点和边有着不同的特征分布，比如文章有文本特征(Text Feature))而机构则其特征来自于附属该机构的学者，共同作者关系和引用关系显然不同。OAG明显是在不断演化，比如文章发表数目增长，会议关注点的变更(KDD在1990年与数据库更相关)，OAG的图结构中存在大量的节点数目与边的数目。 考虑到这些问题和挑战，我们提出了异质图神经网络，其实现了如下四个特点： 保留节点类型和边类型依赖(node-type dependent)的表示 捕捉网络动态性 避免预定义元路径 可扩展到大规模网络 为了解决图的异质性，我们提出了节点和边类型依赖的注意力机制，模型中所用的异质互注意力(mutual attention) 通过将边$e={s,t}$拆解为元关系三元组$&lt;s节点类型，边类型，t节点类型&gt;$，对这些元关系设置相应的权重矩阵来计算连边上的注意力值，这样每一个不同类型的节点和边都刻意保留其特有的特征空间。与此同时，对于类型不同的节点仍可以在不影响其特征分布的前提下进行传递信息。HGT也可以通过message passing的机制来叠加多层来获得高阶邻居的信息，从某种程度上也是定义了soft meta-path，无需通过预定义元路径，而是利用所提出的注意力机制自动学到隐式元路径服务于下游任务。 同时为了建模图的动态性，我们提出相对时间编码(relative temporal encoding)策略，将不同时刻的所有边全都作为一个整体而不对其基于时间片切分，RTE能够建模任意长度时间下的结构依赖性，并作为到未曾出现的未来时刻。最终，RTE使HGT能自动学习到这种时间依赖于异质图的演变。 同时我们提出HGSampling采样策略以应对大规模的图数据，其核心思想通过采样异质子图使不同类型节点具有相同的比例，而目前的诸如GraphSage, FastGCN, LADIES都会导致一个采样出来的节点类型与边类型 highly imbalanced 的分布。并且HGSampling尽可能使子图紧密使信息得以保留，并可以应用于 arbitrary-size 的异质图。 我们在OAG数据上展现了模型的有效性，该数据集包含1.79亿个节点和20亿条边，并且涵盖了1900到2019年所有的数据（这个规模未曾在异质图使用），实验表明我们的模型比其他SOTA的GNN模型与异质图专用模型效果要好9-21%，并且实验表明该模型可以自动获取隐元路径在不同任务下的权重。 2. PRELIMINARIES2.1 Heterogeneous Graph Mining Heterogeneous Graph：异质图可以定义为一个有向图$G=(\\mathcal{V}, \\mathcal{E}, \\mathcal{A}, \\mathcal{R})$，节点$v \\in \\mathcal{V}$与边$e \\in \\mathcal{E}$都有对应映射函数 $\\tau(v) \\colon V \\rightarrow \\mathcal{A}$与$\\phi(e) \\colon E \\rightarrow \\mathcal{R}$。异质图满足$|\\tau(v)| + |\\phi(e)| &gt;2$。 Meta Relation：对于任意一条边$e=(s,t)$，其元关系(meta relation)可以定义为$&lt;\\tau(s), \\phi(e), \\tau(t)&gt;$，$\\phi(e)^{-1}$则是inverse of $\\phi(e)$. 为了更好的建模这种数据之间的异质性，在异质图中假设两个节点之间可以存在多种元关系。 MetaPath：元路径$\\Phi$ 可以理解为由元关系所组成的序列 $\\mathcal{A}1 \\stackrel{\\mathcal{R}_1}{\\rightarrow} \\mathcal{A}_2 \\stackrel{\\mathcal{R}_2}{\\rightarrow} \\cdots \\stackrel{\\mathcal{R}_L}{\\rightarrow} \\mathcal{A}{L+1}$ (or $\\Phi = \\mathcal{A}1 \\rightarrow \\mathcal{A}_2 \\rightarrow \\cdots \\rightarrow \\mathcal{A}{L+1}$) Dynamic Heterogeneous Graph： 为了建模图的动态性，我们认为每一条边的时间是固定在，取决于其生成边的时刻，于是可以对每一条边$e=(s,t)$赋予一个时间戳$T$，表明$s$在$T$时刻与$t$相连，而对于节点来说，一个可以赋予多个时间戳，例如WWW@1994就是第一节WWW会议，其主要关注点在 Internet Protocol 和 Web Infrastructure, 而对于WWW@2020来说关注 social analysis, computing search, IR, privacy and society 等等 2.2 Graph Neural Network 2.3 Heterogeneous GNNs RGCN: 对每一个边类型设计一个独特的投射矩阵(linear projection weight) HetGNN: 利用多个RNN对不同类型的节点进行提取信息并融合 multi-model 的信息 HAN: 利用多个attention对通过元路径获得的邻接矩阵进行分别聚合并融合 这些模型通常只用节点类型或边类型来设计权重矩阵，但对于出现频率很低的边，很难学习到准确的relation-specific的权重矩阵，所以本文的核心在于参数共享 每条边$e=(s,t)$都有与该边对应的元关系（meta-relation) $&lt;\\tau(s), \\phi(e), \\tau(t)&gt;$，边与边之间存在一定联系，例如：对于“第一作者”与“第二作者”这两种边，其源(source)节点和目标(target)节点均分别为 Author 和 Paper。 3. HETEROGENEOUS GRAPH TRANSFORMER 该模型可以被拆分为四个模块，异质互注意力模块、异质消息传递模块、类型聚集模块、RTE模块 3.1 Heterogeneous Mutual Attention先基于如下公式进行计算对于传统图注意力机制GAT其假设节点处于同一个特征分布，将忽略了异质性，本文我们通过基于元关系$&lt;\\tau(s), \\phi(e), \\tau(t)&gt;$这个三元组，类似于Transformer的结构将目标节点$t$作为Query向量，源节点$s$作为Key向量，同时我们对每一个元关系都有专用的投射矩阵，目的是最大程度的建模分布的差异性 Transformer Attention Heterogeneous Graph Transformer传统的Transformer结构利用dot-product进行计算attention系数（Query Vector 和 Key Vector之间的相似度），而本文为了建模边的异质性，则使用edge-specific的矩阵来捕捉不同的语义关系。$\\mu_{&lt;\\tau(s), \\phi(e), \\tau(t)&gt;}$可以理解为对该meta-relation的一个权重（学习得到） 3.2 Heterogeneous Message Passing和上一个模块类似，这里先对每个源节点特征进行投射相应特征空间，然后利用边的权重矩阵进行构建边的依赖性。其实本质思想就是，一个节点特征，首先先过一个节点类型对应的投射矩阵，然后再过相应的边类型对应的投射矩阵，来建立这种节点与边的异质性。 3.3 Target-Specific Aggregation此时获得到的$\\widetilde{H}^{(l)}[t]$是经过源节点类型和边类型矩阵线性组合后的结果对聚合的信息进行转化成目标节点对应的特征分布，然后一个短路连接汇聚自身信息。 Summary：如果直接那meta-relation每个构造一个矩阵的话，那么参数不能得到共享，明显很多有meta-relation之间相关性都忽视了，而现在这样相同类型共享参数，只是改变了中间关系的投射矩阵，使很多关系数目很少也能得到相应的泛化 3.4 Relative Temporal Encoding 通常构建网络的dynamic性质一般进行时间片切割，对切割的时间片单独作为一个图，但是这样不同时间片上的结构依赖会丢失，并且$T_1$时间的节点表示可能依赖于$T_n$时间片上的边。所以更恰当的做法应当使用一个动态图将所有边进行保留，是不同时间片上的节点和边可以交互。其中$\\Delta T(t,s)=T(t)-T(s)$为相对时间差，$i$表示维度，为了使RTE模块的表示能和节点嵌入相加，使其和$d$(即节点表示维度)相同，每个维度都是不同频率的$sin/cos$构建的正弦曲线（类似于在这个函数上进行采点） We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 4. WEB-SCALE HGT TRAINING4.1 HGSampling核心思想就是对目标节点的邻居中每一种类型$\\tau$维护一个bucket $B[\\tau]$，每次基于各个bucket中的节点的重要性（相关度、采样的概率）进行采相同数目的节点，并且有以下优点： 使每种类型的节点和边数目相近 使采样获得的子图尽可能紧密（Dense)以减少信息损失，并且可以减少采样方差 4.2 Inductive Timestamp Assignment引入一些先验信息，异质图中存在一些节点明显具有固定时间戳，如文章取决于发表日期，这类节点作为“事件”节点，也有类似会议节点时间戳应该基于与其相连的事件节点（采样）的时刻。所以对于这种类型节点，其初始时间戳为空，表明其可以与任意时间相连，这样在采样时就可以自适应的获取对应的时间戳。 5 EVALUATION5.1 Web-Scale Datasets主要体现数据量数据规模的大 The Field nodes in OAG are categorized into six levels from $L_0$ to $L_5$, which are organized with a hierachical tree.Claming all these element are differentiated: author orders venue types self-loop relation type $\\phi$ with an reverse relation type $\\phi^{-1}$ 5.2 Experimental SetupTasks: 三个节点分类任务，Paper-Field(L1)、Paper-Field(L2)、Paper-Venue。分别表示文章属于一级（二级）领域中哪一个、已经文章发表对应的会议。类似DBLP中判断文章的类型(data mining, machine learning, etc.)","link":"/heterogeneous-graph-transformer.html"},{"title":"Minimum Increment to Make Array Unique","text":"leetcode problem 945 Level: Medium Description: Given an array of integers A, a move consists of choosing any A[i], and incrementing it by 1. Tags: Array Straight Forward sort the original array iterate the sorted array while adding the cost of each element in order to satisfy the given requirement. 123456789101112class Solution {public: int minIncrementForUnique(vector&lt;int&gt;&amp; A) { sort(A.begin(), A.end()); int res=0, next=0; for(auto a: A){ res += max(next-a, 0); next = max(a, next) + 1; } return res; }}; Time Complexity: O(Nlog(N)) Space Complexity: O(1) O(Klog(k)) by using mapSame idea as the first one, only difference is that now we count occurrences of each elements and calculate total cost for the same element. Time Complexity: O(N+Klog(K)) using TreeMap in C+++ Space Complexity: O(K) 1234567891011121314class Solution {public: int minIncrementForUnique(vector&lt;int&gt;&amp; A) { map&lt;int,int&gt; map; for(auto e: A) map[e]+=1; int res=0, next=0; for(auto it: map){ res += max(next-it.first, 0)*it.second + (it.second-1)*it.second/2; next = max(next, it.first) + it.second; } return res; }};","link":"/leetcode-945.html"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Unsupervised","slug":"Unsupervised","link":"/tags/Unsupervised/"},{"name":"Deep Clustering","slug":"Deep-Clustering","link":"/tags/Deep-Clustering/"},{"name":"GNN","slug":"GNN","link":"/tags/GNN/"},{"name":"Autoencoder","slug":"Autoencoder","link":"/tags/Autoencoder/"},{"name":"math","slug":"math","link":"/tags/math/"},{"name":"deeplearning","slug":"deeplearning","link":"/tags/deeplearning/"},{"name":"论文分享","slug":"论文分享","link":"/tags/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Blog","slug":"Blog","link":"/tags/Blog/"},{"name":"聚类外部指标","slug":"聚类外部指标","link":"/tags/%E8%81%9A%E7%B1%BB%E5%A4%96%E9%83%A8%E6%8C%87%E6%A0%87/"},{"name":"NMI","slug":"NMI","link":"/tags/NMI/"},{"name":"ARI","slug":"ARI","link":"/tags/ARI/"},{"name":"Purity","slug":"Purity","link":"/tags/Purity/"},{"name":"F-Measures","slug":"F-Measures","link":"/tags/F-Measures/"},{"name":"Random Walk","slug":"Random-Walk","link":"/tags/Random-Walk/"},{"name":"Markov Chain","slug":"Markov-Chain","link":"/tags/Markov-Chain/"},{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"}],"categories":[{"name":"论文分享","slug":"论文分享","link":"/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"},{"name":"机器学习","slug":"机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"随笔","slug":"随笔","link":"/categories/%E9%9A%8F%E7%AC%94/"},{"name":"数学","slug":"机器学习/数学","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E5%AD%A6/"},{"name":"聚类指标","slug":"机器学习/聚类指标","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB%E6%8C%87%E6%A0%87/"},{"name":"algorithm","slug":"algorithm","link":"/categories/algorithm/"},{"name":"leetcode","slug":"algorithm/leetcode","link":"/categories/algorithm/leetcode/"}]}