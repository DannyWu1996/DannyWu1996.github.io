{"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"Structral Deep Clustering Network","text":"Structral Deep Clustering NetworkWWW2020本文为尝试将结构化信息建模在深度聚类任务中，并提出了全新的结构化深度聚类模型，其中包括一个DNN模块、GCN模块和双重自监督模块，并通过一个delivery operator成功将autoencoder和GCN所学到的表示结合在一起，并从理论上证明了其有效性。Paper ABSTRACT深度聚类算法目前取得了不错的效果，但是大多数依赖于深度模型的表征能力，比如自编码器（autoencoder），这类深度聚类方法的能力在于提取数据本身的表示，而缺少对数据的结构的建模，本文基于GCN提出Structral Deep Clustering Network(SDCN)，利用delivery operator将自编码器中学到的表示传入对应GCN层中，然后通过一个双自监督机制将两个模型进行整合并一起更新，从而将结构信息融合到自编码器所生成的表示中。本文还对所提出的delivery operator进行理论分析，在这个算子下，GCN作为自编码器的高阶图正则化约束（high-order graph regularization constraint），自编码器可以缓解GCN过平滑（over-smoothing）的问题。在多个任务上取得了SOTA的效果。 INTRODUCTION聚类任务实质自安于将相似样本归于同一类，深度聚类的基本思想在于将聚类任务和深度学习的表征能力结合，通过学习更有效的数据表示来提升聚类效果，类似的方法只考虑数据本身的特征却忽略了数据的结构信息。文中提出在深度聚类中结合结构化信息有以下两个难点： 哪些结构化信息需要被考虑？低阶与高阶邻居 结构化信息和深度聚类之间的关系？DNN每层与多种类型结构信息之间的联系为了捕获数据的结构化信息，我们首先用KNN构图，用GCN去捕获KNN图中的低阶和高阶的结构信息，从而获得GCN-specific表示。为了在深度聚类中引入结构化信息，我们利用autoencoder模块捕捉数据的表示，并提出delivery operator将从GCN中或得到的表示与autoencoder获得的表示进行整合，同时我们证明在delivery operator的帮助下，GCN可以为autoencoder提供二阶图正则化，autoencoder则减轻了GCN过平滑的问题。由于两个模块均能输出数据表示，我们提出双自监督模块对GCN和autoencoder模块进行统一训练，实验表明提出的SDCN在六个数据集上取得了SOTA效果。 THE PROPOSED MODEL KNN Graph这一部分就是利用KNN对每个样本进行计算相似度，对每个样本取top-K的邻居生成相应的边。我们用了两种方法计算样本之间的相似度： 1) Heat Kernel:$$ S_{ij}=e^{- \\frac{||x_{i}-x_{j}||^{2}}{t}}$$2) Dot-product:$$ S_{ij}=x_{j}^T * x_{i} $$ DNN Module为了通用性，在本文中我们仅使用basic autoencoder从raw data中获取表示以适应不同的数据特征。最后将decoder的输出与原始数据的输入进行构造重构误差 GCN ModuleL层autoencoder获得到多种数据表示$\\mathbf{H}^1, \\mathbf{H}^2, \\cdots, \\mathbf{H}^L$，但是忽略了样本之间的联系，本模块用GCN来捕捉结构信息，我们认为每一层autoencoder的输出都包含了不同的信息且能够对原始数据进行重构，于是使用一个超参$\\epsilon=0.5$对两个模型所获取到的表示进行融合将最后一层的输出投射到相应的label维度，然后利用softmax进行归一化。 Dual Self-Supervised Module我们虽然已经将autoencoder和GCN整合在一起，但是还未针对深度聚类任务进行构造，通常autoencoder适用于无监督任务，而GCN则应用于半监督任务，因此不能直接应用到聚类问题上。我们提出一个双自监督模块，对两者进行整合，并进行端到端训练用于聚类任务。对于$i-th$样本和$j-th$集簇，使用学生t-分布作为相似度计算核函数，从而计算节点与簇中央节点（预训练autoencoder的表示经过K-Means获得）的相似度。（注意$\\mathbf{H}$是从autoencoder中学习到的表示，而$\\mathbf{Z}$是GCN模块输出的表示）$q_{ij}$可以视为样本$i$分到$j$集簇的概率，我们将$Q=[q_{ij}]$作为所有样本指派的分布，然后我们将利用KL Divergence对表示进行更新使$Q$分布趋近于$P$分布（自监督，因为P分布是从Q分布中获得，又监督Q分布的更新过程）$P$分布的目的让节点表示趋近于其集簇中央，是集簇更紧密，$f_{j}=\\sum_{i}{q_{ij}}$对于训练GCN模块，如果将上面集簇的指派作为真实标签，容易带来噪音和Trivial Solution，使整个模型训练崩塌，因此我们可以让GCN最后一层输出（分类层）的$Z$分布同时也趋近与$P$分布，同样也利用KL Divergence。这里使用KL散度作为目标函数有两个好处： 传统的多分类损失函数相比，KL散度可以更缓和的更新整个模型，避免嵌入有过大的波动 GCN和DNN整合使用同一个优化目标，让训练过程更加一致 模型最终的损失函数如下所示：最后将模型GCN模块的最终输出Z作为最终的聚类结果，因为GCN中包含了结构信息和数据自身特征 Theory Analysis暂略 Experiments我们对6个数据集进行了实验，包括USPS、HHAR、Reuters、ACM、DBLP、Citeseer，对比了6个baseline，并在四个指标上进行了实验，实验结果如下所示 Analysis https://dannywu1996.github.io/2020/02/27/structral-deep-clustering-network/#moreof Clustering Results 对于ACM、DBLP、Citeseer直接用原始图作为输入，可以从表格中看到提出的SDCN在4个指标上都要超越其他baseline， SDCN普遍高于$SDCN_Q$，表明结构化信息的作用，但同时也依赖于一个噪音少的初始网络 对于已经有网络结构的，GCN类的方法要优于基于自编码器的方法，否则相反。构造好的网络很重要！！ Analysis of Different Propagation Layers通过固定住autoencoder的层数4层去观察，GCN的层数对模型性能的影响： 增加GCN层数提升了聚类的性能，并且4层GCN效果最好，因为自编码器每一层所捕获的信息不同，4层GCN也能保存对应每一层自编码器所学到的表示 3层效果普遍比如2层，因为3层GCN用使用到第二层自编码器的结果，是一个转移层，信息有所丢失，且2层GCN的过平滑效果不严重K-sensitivity Analysis不同的$K$值效果有差异，但是普遍在$k=3$或$k=5$时取得最好的效果，$k=1$时缺少结构信息，$k=10$时社区之间重叠程度过高。","link":"/structral-deep-clustering-network.html"},{"title":"机器学习相关数学笔记","text":"机器学习中所涉及到数学知识接下来持续更新会对看论文过程中所遇到的一些不理解的数学知识进行汇总，方便以后进行查阅 Contraction Mapping Mutual Information Contraction Mapping一个收缩映射（contraction mapping）可以看做在度量空间（metric space） $(M,d)$中从$M$映射到自己本身的函数$f$，其满足对于任意$x \\in M, y \\in M$,$$d(f(x), f(y)) \\le kd(x,y), \\tag{1}$$其中$0 \\le k \\le 1$，$k$最小的那个值被叫做该映射$f$的Lipschitz constant。同样这个概念可以推广到任意两个度量空间$(M, d), (N, d’)$上，对于$f\\colon M\\rightarrow N$，$f$为收缩映射时满足存在一个常量$k&lt;1$，使所有$x,y \\in M$满足$$d’(f(x), f(y)) \\le kd(x,y) \\tag{2}$$ Mutual Information Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable. 总的来说互信息就是为了衡量两个随机变量之间的依赖性，对于随机变量$X$和$Y$，他们之间的互信息$I(X;Y)$可以用以下公式来计算：$$I(X;Y)=H(X)-H(X|Y) \\tag{3}$$其中$H(X)$为随机变量$X$的熵(entropy)，$H(X|Y)$为给定随机变量$Y$后$X$的条件熵，显然如果两个随机变量互相独立，则其互信息为$0$，给定$Y$的信息不会对$X$减少任何不确定性，互信息越大，意味着两个随机变量之间的依赖程度越高。","link":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%95%B0%E5%AD%A6.html"},{"title":"hexo_profile","text":"第一次创建自己的博客记录一下最终还是决定把自己的博客给建立起来，也希望是对自己的一个督促吧，也开始学习一下markdown的语法，加油","link":"/hexo-profile.html"},{"title":"聚类外部指标总结","text":"4种外部指标总结项目中要用到NMI和ARI两个指标，但是一直不太理解这两个指标究竟侧重点是什么，正好做一下记录，顺便对另外两种外部指标也进行一个调研(Purity and F-Measures） Purity的定义Purity的纯净度其实就是对划定好的聚类结果，对每个cluster中数目最多的一类作为该集簇的类别，从而计算样本正确分配到相应集簇的数目 但如果集簇的数目很多时，很容易取得较高purity，比如如果每一个样本自为一类则纯净度为1，因此我们不能用纯净度作为集簇的质量和数目的tradeoff。 NMI的定义nmi主要是用来通过互信息和熵值得比值，对于$\\Omega$为划分的集合，$\\mathbb{C}$为总共的类别， ARI的定义 rule of thumb is : Use ARI when the ground truth clustering has large equal sized clusters 介绍ARI之前先介绍一个Rand Index(RI), 假设一个集合$S={o_1, \\cdots, o_n}$和两个划分$X={X_1, \\cdots, X_r}$与$Y={Y_1, \\cdots, Y_s}$，可以获得以下四个统计量: $a$ 用来表示集合$S$中节点对若在$X$中属于同一划分同时在$Y$中也属于同一划分的数目 $b$ 用来表示集合$S$中节点对若在$X$中属于不同划分同时在$Y$中也属于不同划分的数目 $c$ 用来表示集合$S$中节点对若在$X$中属于同一个划分同时在$Y$中属于不同划分的数目 $d$ 用来表示集合$S$中节点对若在$X$中属于不同划分同时在$Y$中也属于同一划分的数目 则RI指标为:$$RI=\\frac{a+b}{a+b+c+d}=\\frac{a+b}{C_{n}^{2}}$$对于ARI来说其公式首先涉及到一个contigency table表格中的$n_{ij}$代表公共元素个数对于子集$X_i$与$Y_j$: $|X_{i} \\cup Y_{j}|$, 随后ARI的计算公式为","link":"/difference-between-nmi-and-ari.html"},{"title":"random_walks_and_markov_chains","text":"Random Walks and Markov ChainsRelated Concept strongly connected: 图中每个节点均能被其他节点可达（reachabel)","link":"/random-walks-and-markov-chains.html"},{"title":"Heterogeneous Graph Transformer论文分享","text":"Heterogeneous Graph Transformer本文提出了异质图上transformer结构HGT，保留节点特征分布异质性，提出元关系，并使用基于元关系计算相似度替代点乘，模型中利用相对时间差对动态图进行建模并提出HGSampling方法以解决规模大的图结构，在多个节点分类与链接预测任务上相较其他HGNN取得SOTA效果。paper ABSTRACT近年来提出的GNN多应用于同质图，考虑节点和边都属于同一种类型，而我们提出Heterogeneous Graph Transformer (HGT)模型可以解决Web-scale的异质图，为了捕获其异质性，我们设计了节点类型与边类型依赖的参数已使用异质图注意力机制，使模型维护了不同类型节点和边的专用表示。为了应对动态异质图，我们提出了相对时间编码机制来捕捉任意时刻的动态结构依赖性。同时我们设计了异质图mini-batch采样算法HGSampling使模型更高效。多种下游任务表明我们模型超越多个SOTA GNN baseline。 Introduction传统的异质图方法，如PathSim，metapath2vec等都需要通过预先定义meta-path元路径来建模异质结构，而近来也有将GNN应用到异质图中的工作，但他们也有以下问题： 大多数GNN也依赖于预先定于元路径 通常这些GNN模型假设所有节点和边特征均属于同一个特征空间，或者为每一种类型设定独有的投射矩阵，使其无法有效捕捉异质图上的性质 通常忽略了异质图的动态性 没法应用于大规模的异质图中 比如OAG中每个节点和边有着不同的特征分布，比如文章有文本特征而机构则其特征来自于其学者，并且共同作者关系和引用关系显然不同。OAG明显是在不断演化，比如发表数目增长，会议关注点的变更，且大量的节点数目与边的数目。考虑到这些问题和挑战，我们提出了异质图神经网络，其实现了保留节点和边依赖的表示、捕捉网络动态性、避免元路径定义、可扩展到大规模网络等四个功能。为了解决图的异质性，我们提出了节点和边类型依赖度额注意力机制，模型中所用的异质互注意力(mutual attention) 基于将边$e={s,t}$拆解为元关系三元组$&lt;s节点类型，边类型，t节点类型&gt;$. 于是我们通过这些元关系设置相应的权重矩阵来计算连边上的注意力值，这样每一个不同类型的节点和边都刻意保留其特有的特征空间。与此同时，对于类型不同且相连的节点仍可以在不影响其特征分布的前提下进行传递信息。HGT也可以通过叠加多层来获得高阶邻居的信息，同时可以被视为一种”soft”元路径，即便无需设置元路径，通过所提出的注意力机制也可以自动学到隐式元路径服务于下游任务。同时为了建模图的动态性，我们提出相对时间编码（RTE）策略，将不同时刻的所有边全都作为一个整体而不对其基于时间片切分，RTE能够建模任意长度时间下的结构依赖性，并作为到未曾出现的未来时刻。最终，RTE使HGT能自动学习到这种时间依赖于异质图的演变。同时我们提出HGSampling采样策略以应对大规模的图数据，其衷心思想通过采样异质子图使不同类型节点具有相同的比例，同时尽可能使子图紧密使信息得以保留，也使HGT可以应用于任意大小的异质图。我们在OAG数据上展现了模型的有效性，该数据集包含1.79亿个节点和20亿条边，并且涵盖了1900到2019年所有的数据（这个规模未曾在异质图使用），实验表明我们的模型比其他SOTA的GNN模型与异质图专用模型效果要好9-21%，并且实验表明该模型可以自动获取隐元路径在不同任务下的权重。 PRELIMINARIES Heterogeneous Graph：异质图定义不说了 Meta Relation：参照原文 Dynamic Heterogeneous Graph： 引入不同个时间片，给边赋予时间，详情参照原文定义 HETEROGENEOUS GRAPH TRANSFORMER该模型可以被拆分为三个模块，异质互注意力模块、异质消息传递模块、类型聚集模块 Heterogeneous Mutual Attention先基于如下公式进行计算","link":"/heterogeneous-graph-transformer.html"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Unsupervised","slug":"Unsupervised","link":"/tags/Unsupervised/"},{"name":"Deep Clustering","slug":"Deep-Clustering","link":"/tags/Deep-Clustering/"},{"name":"GNN","slug":"GNN","link":"/tags/GNN/"},{"name":"Autoencoder","slug":"Autoencoder","link":"/tags/Autoencoder/"},{"name":"math","slug":"math","link":"/tags/math/"},{"name":"deeplearning","slug":"deeplearning","link":"/tags/deeplearning/"},{"name":"论文分享","slug":"论文分享","link":"/tags/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Blog","slug":"Blog","link":"/tags/Blog/"},{"name":"聚类外部指标","slug":"聚类外部指标","link":"/tags/%E8%81%9A%E7%B1%BB%E5%A4%96%E9%83%A8%E6%8C%87%E6%A0%87/"},{"name":"NMI","slug":"NMI","link":"/tags/NMI/"},{"name":"ARI","slug":"ARI","link":"/tags/ARI/"},{"name":"Purity","slug":"Purity","link":"/tags/Purity/"},{"name":"F-Measures","slug":"F-Measures","link":"/tags/F-Measures/"},{"name":"Random Walk","slug":"Random-Walk","link":"/tags/Random-Walk/"},{"name":"Markov Chain","slug":"Markov-Chain","link":"/tags/Markov-Chain/"}],"categories":[{"name":"论文分享","slug":"论文分享","link":"/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"},{"name":"机器学习","slug":"机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"随笔","slug":"随笔","link":"/categories/%E9%9A%8F%E7%AC%94/"},{"name":"数学","slug":"机器学习/数学","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E5%AD%A6/"},{"name":"聚类指标","slug":"机器学习/聚类指标","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB%E6%8C%87%E6%A0%87/"}]}