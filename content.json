{"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"Structral Deep Clustering Network","text":"Structral Deep Clustering NetworkWWW2020本文为尝试将结构化信息建模在深度聚类任务中，并提出了全新的结构化深度聚类模型，其中包括一个DNN模块、GCN模块和双重自监督模块，并通过一个delivery operator成功将autoencoder和GCN所学到的表示结合在一起，并从理论上证明了其有效性。Paper ABSTRACT深度聚类算法目前取得了不错的效果，但是大多数依赖于深度模型的表征能力，比如自编码器（autoencoder），这类深度聚类方法的能力在于提取数据本身的表示，而缺少对数据的结构的建模，本文基于GCN提出Structral Deep Clustering Network(SDCN)，利用delivery operator将自编码器中学到的表示传入对应GCN层中，然后通过一个双自监督机制将两个模型进行整合并一起更新，从而将结构信息融合到自编码器所生成的表示中。本文还对所提出的delivery operator进行理论分析，在这个算子下，GCN作为自编码器的高阶图正则化约束（high-order graph regularization constraint），自编码器可以缓解GCN过平滑（over-smoothing）的问题。在多个任务上取得了SOTA的效果。 INTRODUCTION聚类任务实质自安于将相似样本归于同一类，深度聚类的基本思想在于将聚类任务和深度学习的表征能力结合，通过学习更有效的数据表示来提升聚类效果，类似的方法只考虑数据本身的特征却忽略了数据的结构信息。文中提出在深度聚类中结合结构化信息有以下两个难点： 哪些结构化信息需要被考虑？低阶与高阶邻居 结构化信息和深度聚类之间的关系？DNN每层与多种类型结构信息之间的联系为了捕获数据的结构化信息，我们首先用KNN构图，用GCN去捕获KNN图中的低阶和高阶的结构信息，从而获得GCN-specific表示。为了在深度聚类中引入结构化信息，我们利用autoencoder模块捕捉数据的表示，并提出delivery operator将从GCN中或得到的表示与autoencoder获得的表示进行整合，同时我们证明在delivery operator的帮助下，GCN可以为autoencoder提供二阶图正则化，autoencoder则减轻了GCN过平滑的问题。由于两个模块均能输出数据表示，我们提出双自监督模块对GCN和autoencoder模块进行统一训练，实验表明提出的SDCN在六个数据集上取得了SOTA效果。 THE PROPOSED MODEL KNN Graph这一部分就是利用KNN对每个样本进行计算相似度，对每个样本取top-K的邻居生成相应的边。我们用了两种方法计算样本之间的相似度： 1) Heat Kernel:$$ S_{ij}=e^{- \\frac{||x_{i}-x_{j}||^{2}}{t}}$$2) Dot-product:$$ S_{ij}=x_{j}^T * x_{i} $$ DNN Module为了通用性，在本文中我们仅使用basic autoencoder从raw data中获取表示以适应不同的数据特征。最后将decoder的输出与原始数据的输入进行构造重构误差 GCN ModuleL层autoencoder获得到多种数据表示$\\mathbf{H}^1, \\mathbf{H}^2, \\cdots, \\mathbf{H}^L$，但是忽略了样本之间的联系，本模块用GCN来捕捉结构信息，我们认为每一层autoencoder的输出都包含了不同的信息且能够对原始数据进行重构，于是使用一个超参$\\epsilon=0.5$对两个模型所获取到的表示进行融合将最后一层的输出投射到相应的label维度，然后利用softmax进行归一化。 Dual Self-Supervised Module我们虽然已经将autoencoder和GCN整合在一起，但是还未针对深度聚类任务进行构造，通常autoencoder适用于无监督任务，而GCN则应用于半监督任务，因此不能直接应用到聚类问题上。我们提出一个双自监督模块，对两者进行整合，并进行端到端训练用于聚类任务。对于$i-th$样本和$j-th$集簇，使用学生t-分布作为相似度计算核函数，从而计算节点与簇中央节点（预训练autoencoder的表示经过K-Means获得）的相似度。（注意$\\mathbf{H}$是从autoencoder中学习到的表示，而$\\mathbf{Z}$是GCN模块输出的表示）$q_{ij}$可以视为样本$i$分到$j$集簇的概率，我们将$Q=[q_{ij}]$作为所有样本指派的分布，然后我们将利用KL Divergence对表示进行更新使$Q$分布趋近于$P$分布（自监督，因为P分布是从Q分布中获得，又监督Q分布的更新过程）$P$分布的目的让节点表示趋近于其集簇中央，是集簇更紧密，$f_{j}=\\sum_{i}{q_{ij}}$对于训练GCN模块，如果将上面集簇的指派作为真实标签，容易带来噪音和Trivial Solution，使整个模型训练崩塌，因此我们可以让GCN最后一层输出（分类层）的$Z$分布同时也趋近与$P$分布，同样也利用KL Divergence。这里使用KL散度作为目标函数有两个好处： 传统的多分类损失函数相比，KL散度可以更缓和的更新整个模型，避免嵌入有过大的波动 GCN和DNN整合使用同一个优化目标，让训练过程更加一致 模型最终的损失函数如下所示：最后将模型GCN模块的最终输出Z作为最终的聚类结果，因为GCN中包含了结构信息和数据自身特征 Theory Analysis暂略 Experiments我们对6个数据集进行了实验，包括USPS、HHAR、Reuters、ACM、DBLP、Citeseer，对比了6个baseline，并在四个指标上进行了实验，实验结果如下所示 Analysis https://dannywu1996.github.io/2020/02/27/structral-deep-clustering-network/#moreof Clustering Results 对于ACM、DBLP、Citeseer直接用原始图作为输入，可以从表格中看到提出的SDCN在4个指标上都要超越其他baseline， SDCN普遍高于$SDCN_Q$，表明结构化信息的作用，但同时也依赖于一个噪音少的初始网络 对于已经有网络结构的，GCN类的方法要优于基于自编码器的方法，否则相反。构造好的网络很重要！！ Analysis of Different Propagation Layers通过固定住autoencoder的层数4层去观察，GCN的层数对模型性能的影响： 增加GCN层数提升了聚类的性能，并且4层GCN效果最好，因为自编码器每一层所捕获的信息不同，4层GCN也能保存对应每一层自编码器所学到的表示 3层效果普遍比如2层，因为3层GCN用使用到第二层自编码器的结果，是一个转移层，信息有所丢失，且2层GCN的过平滑效果不严重K-sensitivity Analysis不同的$K$值效果有差异，但是普遍在$k=3$或$k=5$时取得最好的效果，$k=1$时缺少结构信息，$k=10$时社区之间重叠程度过高。","link":"/structral-deep-clustering-network.html"},{"title":"机器学习相关数学笔记","text":"机器学习中所涉及到数学知识接下来持续更新会对看论文过程中所遇到的一些不理解的数学知识进行汇总，方便以后进行查阅 Contraction Mapping Mutual Information Contraction Mapping一个收缩映射（contraction mapping）可以看做在度量空间（metric space） $(M,d)$中从$M$映射到自己本身的函数$f$，其满足对于任意$x \\in M, y \\in M$,$$d(f(x), f(y)) \\le kd(x,y), \\tag{1}$$其中$0 \\le k \\le 1$，$k$最小的那个值被叫做该映射$f$的Lipschitz constant。同样这个概念可以推广到任意两个度量空间$(M, d), (N, d’)$上，对于$f\\colon M\\rightarrow N$，$f$为收缩映射时满足存在一个常量$k&lt;1$，使所有$x,y \\in M$满足$$d’(f(x), f(y)) \\le kd(x,y) \\tag{2}$$ Mutual Information Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable. 总的来说互信息就是为了衡量两个随机变量之间的依赖性，对于随机变量$X$和$Y$，他们之间的互信息$I(X;Y)$可以用以下公式来计算：$$I(X;Y)=H(X)-H(X|Y) \\tag{3}$$其中$H(X)$为随机变量$X$的熵(entropy)，$H(X|Y)$为给定随机变量$Y$后$X$的条件熵，显然如果两个随机变量互相独立，则其互信息为$0$，给定$Y$的信息不会对$X$减少任何不确定性，互信息越大，意味着两个随机变量之间的依赖程度越高。","link":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%95%B0%E5%AD%A6.html"},{"title":"hexo_profile","text":"第一次创建自己的博客记录一下最终还是决定把自己的博客给建立起来，也希望是对自己的一个督促吧，也开始学习一下markdown的语法，加油","link":"/hexo-profile.html"},{"title":"聚类外部指标总结","text":"4种外部指标总结项目中要用到NMI和ARI两个指标，但是一直不太理解这两个指标究竟侧重点是什么，正好做一下记录，顺便对另外两种外部指标也进行一个调研(Purity and F-Measures） Purity的定义Purity的纯净度其实就是对划定好的聚类结果，对每个cluster中数目最多的一类作为该集簇的类别，从而计算样本正确分配到相应集簇的数目 但如果集簇的数目很多时，很容易取得较高purity，比如如果每一个样本自为一类则纯净度为1，因此我们不能用纯净度作为集簇的质量和数目的tradeoff。 NMI的定义nmi主要是用来通过互信息和熵值得比值，对于$\\Omega$为划分的集合，$\\mathbb{C}$为总共的类别， ARI的定义 rule of thumb is : Use ARI when the ground truth clustering has large equal sized clusters 介绍ARI之前先介绍一个Rand Index(RI), 假设一个集合$S={o_1, \\cdots, o_n}$和两个划分$X={X_1, \\cdots, X_r}$与$Y={Y_1, \\cdots, Y_s}$，可以获得以下四个统计量: $a$ 用来表示集合$S$中节点对若在$X$中属于同一划分同时在$Y$中也属于同一划分的数目 $b$ 用来表示集合$S$中节点对若在$X$中属于不同划分同时在$Y$中也属于不同划分的数目 $c$ 用来表示集合$S$中节点对若在$X$中属于同一个划分同时在$Y$中属于不同划分的数目 $d$ 用来表示集合$S$中节点对若在$X$中属于不同划分同时在$Y$中也属于同一划分的数目 则RI指标为:$$RI=\\frac{a+b}{a+b+c+d}=\\frac{a+b}{C_{n}^{2}}$$对于ARI来说其公式首先涉及到一个contigency table表格中的$n_{ij}$代表公共元素个数对于子集$X_i$与$Y_j$: $|X_{i} \\cup Y_{j}|$, 随后ARI的计算公式为","link":"/difference-between-nmi-and-ari.html"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Unsupervised","slug":"Unsupervised","link":"/tags/Unsupervised/"},{"name":"Deep Clustering","slug":"Deep-Clustering","link":"/tags/Deep-Clustering/"},{"name":"GNN","slug":"GNN","link":"/tags/GNN/"},{"name":"Autoencoder","slug":"Autoencoder","link":"/tags/Autoencoder/"},{"name":"math","slug":"math","link":"/tags/math/"},{"name":"deeplearning","slug":"deeplearning","link":"/tags/deeplearning/"},{"name":"论文分享","slug":"论文分享","link":"/tags/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Blog","slug":"Blog","link":"/tags/Blog/"},{"name":"聚类外部指标","slug":"聚类外部指标","link":"/tags/%E8%81%9A%E7%B1%BB%E5%A4%96%E9%83%A8%E6%8C%87%E6%A0%87/"},{"name":"NMI","slug":"NMI","link":"/tags/NMI/"},{"name":"ARI","slug":"ARI","link":"/tags/ARI/"},{"name":"Purity","slug":"Purity","link":"/tags/Purity/"},{"name":"F-Measures","slug":"F-Measures","link":"/tags/F-Measures/"}],"categories":[{"name":"论文分享","slug":"论文分享","link":"/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"},{"name":"机器学习","slug":"机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"随笔","slug":"随笔","link":"/categories/%E9%9A%8F%E7%AC%94/"},{"name":"数学","slug":"机器学习/数学","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E5%AD%A6/"},{"name":"聚类指标","slug":"机器学习/聚类指标","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB%E6%8C%87%E6%A0%87/"}]}